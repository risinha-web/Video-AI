"""
Report generation for video search evaluation results.
"""

import json
from datetime import datetime
from pathlib import Path
from typing import Optional

from .metrics import EvaluationResult


def generate_report(
    result: EvaluationResult,
    output_path: Optional[str] = None,
    format: str = "json",
) -> str:
    """Generate an evaluation report.

    Args:
        result: EvaluationResult from evaluate_results().
        output_path: Optional path to write the report.
        format: Report format ("json" or "markdown").

    Returns:
        Report content as a string.
    """
    if format == "json":
        report = _generate_json_report(result)
    elif format == "markdown":
        report = _generate_markdown_report(result)
    else:
        raise ValueError(f"Unknown format: {format}")

    if output_path:
        Path(output_path).write_text(report)

    return report


def _generate_json_report(result: EvaluationResult) -> str:
    """Generate JSON format report."""
    report_data = {
        "generated_at": datetime.utcnow().isoformat() + "Z",
        "version": "1.0",
        **result.to_dict(),
    }
    return json.dumps(report_data, indent=2)


def _generate_markdown_report(result: EvaluationResult) -> str:
    """Generate Markdown format report."""
    lines = [
        "# Video Search Evaluation Report",
        "",
        f"**Generated:** {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC",
        "",
        "## Overall Score",
        "",
        f"**Total Score: {result.total_score:.2%}**",
        "",
        "## Metrics Breakdown",
        "",
        "| Metric | Score | Weight | Weighted |",
        "|--------|-------|--------|----------|",
    ]

    for metric in result.metrics:
        weighted = metric.score * metric.weight
        lines.append(
            f"| {metric.name} | {metric.score:.2%} | {metric.weight:.0%} | {weighted:.2%} |"
        )

    lines.extend([
        "",
        "## Per-Query Results",
        "",
        "| Query | Precision | Recall | TTFR | Latency |",
        "|-------|-----------|--------|------|---------|",
    ])

    for query_id, qr in result.query_results.items():
        lines.append(
            f"| {query_id} | {qr['precision']:.2%} | {qr['recall']:.2%} | "
            f"{qr['time_to_first_result']:.2f} | {qr['latency_ms']:.0f}ms |"
        )

    if result.errors:
        lines.extend([
            "",
            "## Errors",
            "",
        ])
        for error in result.errors:
            lines.append(f"- {error}")

    lines.extend([
        "",
        "---",
        "*Report generated by AI Interview Evaluation Framework*",
    ])

    return "\n".join(lines)


def print_summary(result: EvaluationResult) -> None:
    """Print a brief summary to stdout."""
    print("\n" + "=" * 50)
    print("EVALUATION SUMMARY")
    print("=" * 50)
    print(f"\nTotal Score: {result.total_score:.2%}")
    print("\nMetrics:")
    for metric in result.metrics:
        bar = "#" * int(metric.score * 20) + "-" * (20 - int(metric.score * 20))
        print(f"  {metric.name:25} [{bar}] {metric.score:.2%}")

    if result.errors:
        print(f"\nErrors ({len(result.errors)}):")
        for error in result.errors[:5]:
            print(f"  - {error}")
        if len(result.errors) > 5:
            print(f"  ... and {len(result.errors) - 5} more")

    print("=" * 50 + "\n")
